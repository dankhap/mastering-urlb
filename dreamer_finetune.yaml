defaults:
  - configs/dreamer
  - agent: dreamer
  - configs: ${configs}
  - override hydra/launcher: submitit_local

# mode
reward_free: false
# task settings
task: walker_stand
# train settings
num_train_frames: 100010
# num_seed_frames: 4000
# eval
eval_every_frames: 10000
num_eval_episodes: 10
# pretrained
snapshot_ts: 100000
snapshot_base_dir: /code/mastering-urlb/pretrained_models/${experiment}
# snapshot_base_dir: /code/mastering-urlb/pretrained_models
custom_snap_dir: none
# replay buffer
replay_buffer_size: 1000000
replay_buffer_num_workers: 4
# misc
seed: 1
device: cuda
save_video: false
save_train_video: false
save_eval_episodes: false
use_tb: true
use_wandb: true
# experiment
experiment: ft
wandb_group: ""
project_name: ???

# log settings
log_every_frames: 1000
recon_every_frames: 100000000 # edit for debug 

# planning
mpc: false
mpc_opt: { iterations : 12, num_samples : 512, num_elites : 64, mixture_coef : 0.05, min_std : 0.1, temperature : 0.5, momentum : 0.1, horizon : 5, use_value: true }

# Pretrained network reuse
init_critic: false
init_actor: true

# Params to test faster FT
num_seed_frames: 4000
pretrain_updates: 0 # how many consecutive updates we do for the world model and the actor on first update run (after seed buffer)
reinit_actor: false # do we re-initialize the actor after collecting the seed buffer
preload_buffer: "" # path for preloaded buffer of possibly expert trajectories
preload_replay: false
expert_steps: 0 # number of expert trajectories to load
wm_train_ratio: 1.0 # ratio between env steps and wm updates
policy_train_ratio: 1.0 # ratio between env steps and wm updates
zero_shot: false

# Fine-tuning ablation
save_ft_model: true

# Dreamer FT
grad_heads: [decoder, reward]
reward_norm: {momentum: 1.0, scale: 1.0, eps: 1e-8}
actor_ent: 1e-4

hydra:
  run:
    dir: /code/mastering-urlb/exp_local/${now:%Y.%m.%d}/${now:%H%M%S}_${agent.name}
  sweep:
    dir: /code/mastering-urlb/exp_sweep/${now:%Y.%m.%d}/${now:%H%M}_${agent.name}_${experiment}
    subdir: ${hydra.job.num}
  launcher:
    timeout_min: 4300
    cpus_per_task: 10
    gpus_per_node: 1
    tasks_per_node: 1
    mem_gb: 160
    nodes: 1
    submitit_folder: /code/mastering-urlb/exp_sweep/${now:%Y.%m.%d}/${now:%H%M}_${agent.name}_${experiment}/.slurm
